\section{Answers}

\subsection*{{\bf Q1)}}
\subsubsection*{{\bf a)}}
The neighbours to be returned by the first and second process would be $\left\{ \left( \left[ 2,1 \right], 0 \right), \left( \left[ 2,2 \right], 0 \right) \right\}$ and $\left\{ \left( \left[ 2,1 \right], 0 \right), \left( \left[ 2,2 \right], 0 \right) \right\}$ respectively.

\subsubsection*{{\bf b)}}
No, all $k$ closest neighbours may randomly be given to a single process. If some of these $k$ closest neighbours  were neglected by this process then not all the $k$ closest neighbours will be used in the final reduction step which may give differing results to the serial k-NN algorithm.

\subsubsection*{{\bf c)}}
\subsubsection*{{\bf i)}}
The code for the k-NN algorithm implemented from scratch is shown below
\inputminted[mathescape,
    linenos,
    numbersep=5pt,
    frame=lines,
    framesep=2mm]{python}{src/KNN_parallel.py}

\subsubsection*{{\bf ii)}}
An example batch file for job submission is shown below
\inputminted[mathescape,
    linenos,
    numbersep=5pt,
    frame=lines,
    framesep=2mm]{bash}{src/batch/KNN_1_thread.sh}
    
Code to training a parallel k-NN algorithm using \texttt{sklearn} is given below
\inputminted[mathescape,
    linenos,
    numbersep=5pt,
    frame=lines,
    framesep=2mm]{python}{src/KNN_demo.py}
    
The times for running the parallel algorithm with different numbers of processes is shown below
\begin{table}[h!!!]
\begin{tabular}{l|c|c|c|c}
Number of threads & 1 & 2 & 3 & 4 \\ \cline{2-5} 
Time (sec)        & $0.03754734$ & $0.02993416$ & $0.0252390$ & $0.01833604$
\end{tabular}
\end{table}

\subsection*{{\bf Q2)}}
\subsubsection*{{\bf a)}}
\subsubsection*{{\bf i)}}
Examining line $6$ of Algorithm \ref{alg:one-perceptron-epoch} and knowing that $\bm{u}$ linearly separates the training set by and margin of $\lambda$
\begin{align*}
    \bm{u} \cdot \bm{w}^{(i,n)} &= \bm{u} \cdot \bm{w}^{([i,n] - 1)} + \bm{u} \left( f (\bm{x}_{i}, y_{i}) -  f (\bm{x}_{i}, \overline{y}) \right) \\
    &\geq \bm{w}^{([i,n] - 1)} + \lambda \\
    &\geq \bm{w}^{([i,n] - 2)} + 2 \lambda \\
    &\geq \bm{w}^{(avg, n-1)} + k_{i,n} \lambda
\end{align*}

\subsubsection*{{\bf ii)}}
Again, using line $6$ of Algorithm \ref{alg:one-perceptron-epoch} and using the fact $\bm{w}^{([i,n] - 1)} \left( f (\bm{x}_{i}, y_{i}) -  f (\bm{x}_{i}, \overline{y}) \right) \leq 0$
\begin{align*}
    \norm{\bm{w}^{(i,n)}}^2 &= \norm{\bm{w}^{([i,n] - 1)}}^2 + \norm{f (\bm{x}_{i}, y_{i}) -  f (\bm{x}_{i}, \overline{y})} + 2 \bm{w}^{([i,n] - 1)} \left( f (\bm{x}_{i}, y_{i}) -  f (\bm{x}_{i}, \overline{y}) \right) \\
    &= \norm{\bm{w}^{([i,n] - 1)}}^2 + R^2 \\
    &= \norm{\bm{w}^{([i,n] - 1)}}^2 + 2R^2 \\
    &= \norm{\bm{w}^{([i,n] - 1)}}^2 + k_{i,n} R^2
\end{align*}

\subsubsection*{{\bf b)}}
Consider a binary classification setting where we have two classes $\left\{ 0,1 \right\}$ the training set $T$ has four samples $\left\{ \left( \bm{x}_{1,1}, y_{1,1} \right), \left( \bm{x}_{1,2}, y_{1,2} \right), \left( \bm{x}_{2,1}, y_{2,1} \right), \left( \bm{x}_{2,2}, y_{2,2} \right) \right\}$. Let $y_{1,1} = y_{2,1} = 0$ and $y_{1,2} = y_{2,2} = 1$. Let $\bm{w} \in \mathbb{R}^{6}$ and using block features, define the feature space as
\begin{align*}
    f (\bm{x}_{1,1}, 0) &= \left[ 1 \; 1 \; 0 \; 0 \; 0 \; 0 \right] \quad & f (\bm{x}_{1,1}, 1) &= \left[ 0 \; 0 \; 0 \; 1 \; 1 \; 0 \right] \\
    f (\bm{x}_{1,2}, 0) &= \left[ 0 \; 1 \; 0 \; 0 \; 0 \; 0 \right] \quad & f (\bm{x}_{1,2}, 1) &= \left[ 0 \; 0 \; 0 \; 0 \; 0 \; 1 \right] \\
    f (\bm{x}_{2,1}, 0) &= \left[ 0 \; 1 \; 1 \; 0 \; 0 \; 0 \right] \quad & f (\bm{x}_{2,1}, 1) &= \left[ 0 \; 0 \; 0 \; 0 \; 1 \; 1 \right] \\
    f (\bm{x}_{2,2}, 0) &= \left[ 1 \; 0 \; 0 \; 0 \; 0 \; 0 \right] \quad & f (\bm{x}_{2,2}, 1) &= \left[ 0 \; 0 \; 0 \; 1 \; 0 \; 0 \right]
\end{align*}
Assuming label $1$ as a tie-breaker we find that $\bm{w}^{1} = \left[ 1 \; 1 \; 0 \; -1 \; -1 \; 0 \right]$ and $\bm{w}^{2} = \left[ 0 \; 1 \; 1 \; 0 \; -1 \; -1 \right]$ for any choice of $\mu$ meaning the mixed weight vector will not separate at all points. If $\mu_1$ and $mu_2$ are both non-zero then all example will be classified as $0$. Furthermore if $\mu_1 = 1$ and $\mu_2 = 0$ then the training sample $(\bm{x}_{2,2}, y_{2,2})$ will be incorrectly classified as $0$. Similarly if $\mu_1 = 0$ and $\mu_2 = 1$ then the training sample $(\bm{x}_{2,2}, y_{2,2})$ will also be incorrectly classified as $0$. However, there is indeed a separating hyperplane for $T$ namely $\bm{w} = \left[ -1 \; 2 \; -1 \; 1 \; -2 \; 1 \right]$.

\subsubsection*{{\bf c)}}
An example batch file for job submission is shown below
\inputminted[mathescape,
    linenos,
    numbersep=5pt,
    frame=lines,
    framesep=2mm]{bash}{src/batch/KNN_1_thread.sh}
    
Code to training a parallel k-NN algorithm using \texttt{sklearn} is given below
\inputminted[mathescape,
    linenos,
    numbersep=5pt,
    frame=lines,
    framesep=2mm]{python}{src/KNN_demo.py}
    
The times for running the parallel algorithm with different numbers of processes is shown below
\begin{table}[h!!!]
\begin{tabular}{l|c|c|c|c}
Number of threads & 1 & 2 & 3 & 4 \\ \cline{2-5} 
Time (sec)        & $0.03754734$ & $0.02993416$ & $0.0252390$ & $0.01833604$
\end{tabular}
\end{table}