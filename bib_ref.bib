@book{HastieTrevor2009EoSL,
series = {Springer series in statistics},
abstract = {Covers supervised learning (prediction) to unsupervised learning. This book contains topics including neural networks, support vector machines, classification trees and boosting.},
publisher = {Springer},
booktitle = {Elements of Statistical Learning},
isbn = {0387848576},
year = {2009},
title = {Elements of Statistical Learning: Data Mining, Inference, and Prediction},
language = {eng},
address = {New York},
author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
keywords = {Data mining ; Supervised learning (Machine learning) ; Mathematical statistics},
}

@inproceedings{ShenshenLiang2010Daeo,
issn = {2158-6985},
abstract = {Recent developments in Graphics Processing Units (GPUs) have enabled inexpensive high performance computing for general-purpose applications. Due to GPU's tremendous computing capability, it has emerged as the co-processor of CPU to achieve a high overall throughput. CUDA programming model provides the programmers adequate C language like APIs to better exploit the parallel power of the GPU. K-nearest neighbor (KNN) is a widely used classification technique and has significant applications in various domains, especially in text classification. The computational-intensive nature of KNN requires a high performance implementation. In this paper, we propose CUKNN, a CUDA-based parallel implementation of KNN. It launches two CUDA kernels, distance calculation kernel and selecting kernel. In the distance calculation kernel, a great number of concurrent CUDA threads are issued, where each thread performs the calculation between the query object and a reference object; in the selecting kernel, threads in a block find the local-k nearest neighbors of the query object concurrently, and then a thread is invoked to find the global-k nearest neighbors out of the queues of local-k neighbors. Various CUDA optimization techniques are applied to maximize the utilization of GPU. We evaluate our implementation by using synthetic dataseis and a real physical simulation dataset. The experimental results demonstrate that CUKNN outperforms the serial KNN on an HP xw8600 workstation significantly, achieving up to 46.7IX speedup on the synthetic dataseis and 42.49X on the physical simulation dataset including I/O cost. It also shows good scalability when varying the number of dimensions of the reference dataset, the number of objects in the reference dataset, and the number of objects in the query dataset.},
pages = {53--60},
publisher = {IEEE},
isbn = {9781424463565},
year = {2010},
title = {Design and evaluation of a parallel k-nearest neighbor algorithm on CUDA-enabled GPU},
language = {eng},
author = {Shenshen Liang and Ying Liu and Cheng Wang and Liheng Jian},
keywords = {Instruction sets ; Computational modeling ; Programming ; Classification algorithms ; Kernel ; Graphics processing unit ; Nearest neighbor searches},
}

@article{MalekiSaeed2017PSGD,
abstract = {Stochastic gradient descent (SGD) is a well known method for regression and
classification tasks. However, it is an inherently sequential algorithm at each
step, the processing of the current example depends on the parameters learned
from the previous examples. Prior approaches to parallelizing linear learners
using SGD, such as HOGWILD! and ALLREDUCE, do not honor these dependencies
across threads and thus can potentially suffer poor convergence rates and/or
poor scalability. This paper proposes SYMSGD, a parallel SGD algorithm that, to
a first-order approximation, retains the sequential semantics of SGD. Each
thread learns a local model in addition to a model combiner, which allows local
models to be combined to produce the same result as what a sequential SGD would
have produced. This paper evaluates SYMSGD's accuracy and performance on 6
datasets on a shared-memory machine shows upto 11x speedup over our heavily
optimized sequential baseline on 16 cores and 2.2x, on average, faster than
HOGWILD!.},
year = {2017},
title = {Parallel Stochastic Gradient Descent with Sound Combiners},
copyright = {http://arxiv.org/licenses/nonexclusive-distrib/1.0},
language = {eng},
author = {Maleki, Saeed and Musuvathi, Madanlal and Mytkowicz, Todd},
}

@inproceedings{10.5555/1857999.1858068,
author = {McDonald, Ryan and Hall, Keith and Mann, Gideon},
title = {Distributed Training Strategies for the Structured Perceptron},
year = {2010},
isbn = {1932432655},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {Perceptron training is widely applied in the natural language processing community for learning complex structured models. Like all structured prediction learning frameworks, the structured perceptron can be costly to train as training complexity is proportional to inference, which is frequently non-linear in example sequence length. In this paper we investigate distributed training strategies for the structured perceptron as a means to reduce training times when computing clusters are available. We look at two strategies and provide convergence bounds for a particular mode of distributed structured perceptron training based on iterative parameter mixing (or averaging). We present experiments on two structured prediction problems -- named-entity recognition and dependency parsing -- to highlight the efficiency of this method.},
booktitle = {Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics},
pages = {456â€“464},
numpages = {9},
location = {Los Angeles, California},
series = {HLT '10}
}