\section{Introduction}
Most computational work done within science is impractical to perform on commercial laptops and desktops, typically due to the extremely high memory and processing demands. Hence, almost every university and industry has its own high-performance computer to carry out such strenuous problems. A lot of research within the realm of Machine Learning benefits from access to high performance machinery as most of them require matrix computation (which can be efficiently carried out on GPU clusters) and can be processed in parallel.\\[1\baselineskip]
In this tutorial we will revisit three models covered in the first few weeks of lectures, these being the k-NN classifier, the perceptron and SDG linear regressor. The serial implementations for theses algorithms maybe become fairly inefficient large data sets, so we shall look at some ways in which these two methods can be decomposed and parallelised.